\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{random}\PYG{o}{,} \PYG{n+nn}{pickle}\PYG{o}{,} \PYG{n+nn}{math}
\PYG{k+kn}{from} \PYG{n+nn}{matrix} \PYG{k+kn}{import} \PYG{n}{Matrix}
\PYG{k+kn}{import} \PYG{n+nn}{activations}
\PYG{k+kn}{from} \PYG{n+nn}{copy} \PYG{k+kn}{import} \PYG{n}{copy}
\PYG{k+kn}{from} \PYG{n+nn}{datalogger} \PYG{k+kn}{import} \PYG{o}{*}

\PYG{k}{class} \PYG{n+nc}{DoubleNeuralNet}\PYG{p}{():} \PYG{c+c1}{\PYGZsh{} Wraps a Main and Target Neural Network together}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{layers}\PYG{p}{,} \PYG{n}{params}\PYG{p}{,} \PYG{n}{load}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loadName}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}DQNetwork\PYGZdq{}}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Constructor for a Double Neural Network}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary} \PYG{o}{=} \PYG{n}{params}

        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{load}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Create brand new values}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork} \PYG{o}{=} \PYG{n}{NeuralNet}\PYG{p}{(}\PYG{n}{layers}\PYG{p}{,} \PYG{n}{params}\PYG{p}{)}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{TargetNetwork} \PYG{o}{=} \PYG{n}{NeuralNet}\PYG{p}{(}\PYG{n}{layers}\PYG{p}{,} \PYG{n}{params}\PYG{p}{)}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ExperienceReplay} \PYG{o}{=} \PYG{n}{Deque}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}ERBuffer\PYGZdq{}}\PYG{p}{])}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DQLEpsilon\PYGZdq{}}\PYG{p}{]}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cumReward} \PYG{o}{=} \PYG{l+m+mf}{0.0}
            
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layerActivation} \PYG{o}{=} \PYG{n}{activations}\PYG{o}{.}\PYG{n}{TanH}\PYG{p}{()}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{finalLayerActivation} \PYG{o}{=} \PYG{n}{activations}\PYG{o}{.}\PYG{n}{SoftMax}\PYG{p}{()}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{LoadState}\PYG{p}{(}\PYG{n}{loadName}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Load values from saved data}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fileName} \PYG{o}{=} \PYG{n}{loadName}
        
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{activations} \PYG{o}{=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layerActivation}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{finalLayerActivation}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Tuple of activations}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchReward} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{maxBatchReward} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchLoss} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataPoints} \PYG{o}{=} \PYG{p}{[]}

                                                        \PYG{c+c1}{\PYGZsh{} BatchReward, MaxBatchReward, PercentageDifference, Step}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actionTracker} \PYG{o}{=} \PYG{n}{DataLogger}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}ActionTracker\PYGZdq{}}\PYG{p}{,} \PYG{p}{[[}\PYG{n+nb}{float}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{],} \PYG{p}{[}\PYG{n+nb}{float}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{],} \PYG{p}{[}\PYG{n+nb}{float}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{],} \PYG{n+nb}{int}\PYG{p}{],} \PYG{k+kc}{False}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{TakeStep}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{worldMap}\PYG{p}{,} \PYG{n}{enemyList}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Takes a step forward in time}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step} \PYG{o}{+=} \PYG{l+m+mi}{1}

        \PYG{c+c1}{\PYGZsh{} Forward Propagation}
        \PYG{n}{agentSurround} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{GetTileVector}\PYG{p}{(}\PYG{n}{worldMap}\PYG{p}{,} \PYG{n}{enemyList}\PYG{p}{)}
        \PYG{n}{postProcessedSurround} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{TileVectorPostProcess}\PYG{p}{(}\PYG{n}{agentSurround}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Retrieve Vector of State info from Agent}
        \PYG{n}{netInput} \PYG{o}{=} \PYG{n}{postProcessedSurround}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
        
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{ForwardPropagation}\PYG{p}{(}\PYG{n}{netInput}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{activations}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Forward Prop the Main Network}

        \PYG{n}{output} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{outputVector}
        \PYG{n}{outputMax} \PYG{o}{=} \PYG{n}{output}\PYG{o}{.}\PYG{n}{MaxInVector}\PYG{p}{()}

        \PYG{c+c1}{\PYGZsh{} Action Taking and Reward}
        \PYG{k}{if} \PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{()} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Epsilon slowly regresses, leaving a greater chance for a random action to be explored}
            \PYG{k}{if} \PYG{n+nb}{type}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{finalLayerActivation}\PYG{p}{)} \PYG{o}{==} \PYG{n}{activations}\PYG{o}{.}\PYG{n}{SoftMax}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Sum softmax distribution values and choose a random action from that set}
                \PYG{n}{action} \PYG{o}{=} \PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}
                \PYG{n}{val} \PYG{o}{=} \PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{()}
                \PYG{n}{totalled} \PYG{o}{=} \PYG{l+m+mi}{0}
                \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{output}\PYG{o}{.}\PYG{n}{order}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]):}
                    \PYG{n}{totalled} \PYG{o}{+=} \PYG{n}{output}\PYG{o}{.}\PYG{n}{matrixVals}\PYG{p}{[}\PYG{n}{i}\PYG{p}{][}\PYG{l+m+mi}{0}\PYG{p}{]}
                    \PYG{k}{if} \PYG{n}{totalled} \PYG{o}{\PYGZgt{}=} \PYG{n}{val}\PYG{p}{:}
                        \PYG{n}{action} \PYG{o}{=} \PYG{n}{i}
                        \PYG{k}{break}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{n}{action} \PYG{o}{=} \PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{outputMax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} Choose best action}

        \PYG{n}{action} \PYG{o}{=} \PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DeepQLearningLayers\PYGZdq{}}\PYG{p}{][}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}

        \PYG{n}{rewardVector} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{GetRewardVector}\PYG{p}{(}\PYG{n}{agentSurround}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DeepQLearningLayers\PYGZdq{}}\PYG{p}{][}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{])}
        \PYG{n}{reward} \PYG{o}{=} \PYG{n}{rewardVector}\PYG{o}{.}\PYG{n}{matrixVals}\PYG{p}{[}\PYG{n}{action}\PYG{p}{][}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} Get reward given action}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cumReward} \PYG{o}{+=} \PYG{n}{reward}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchReward} \PYG{o}{+=} \PYG{n}{reward}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{maxBatchReward} \PYG{o}{+=} \PYG{n}{rewardVector}\PYG{o}{.}\PYG{n}{MaxInVector}\PYG{p}{()[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{}print(reward, rewardVector.MaxInVector()[0], action)}

        \PYG{n}{agent}\PYG{o}{.}\PYG{n}{CommitAction}\PYG{p}{(}\PYG{n}{action}\PYG{p}{,} \PYG{n}{agentSurround}\PYG{p}{,} \PYG{n}{worldMap}\PYG{p}{,} \PYG{n}{enemyList}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Take Action}

        \PYG{c+c1}{\PYGZsh{}self.dataPoints.append([reward, rewardVector.MaxInVector()[0], 0, self.step])}

        \PYG{c+c1}{\PYGZsh{} Epsilon Regression}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{*=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DQLEpisonRegression\PYGZdq{}}\PYG{p}{]} 

        \PYG{c+c1}{\PYGZsh{} Assigning values to tempExperience}
        \PYG{n}{tempExp} \PYG{o}{=} \PYG{n}{Experience}\PYG{p}{()}
        \PYG{n}{tempExp}\PYG{o}{.}\PYG{n}{state} \PYG{o}{=} \PYG{n}{agentSurround} 
        \PYG{n}{tempExp}\PYG{o}{.}\PYG{n}{action} \PYG{o}{=} \PYG{n}{action}
        \PYG{n}{tempExp}\PYG{o}{.}\PYG{n}{reward} \PYG{o}{=} \PYG{n}{rewardVector}
        \PYG{n}{tempExp}\PYG{o}{.}\PYG{n}{stateNew} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{GetTileVector}\PYG{p}{(}\PYG{n}{worldMap}\PYG{p}{,} \PYG{n}{enemyList}\PYG{p}{)}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ExperienceReplay}\PYG{o}{.}\PYG{n}{PushFront}\PYG{p}{(}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{tempExp}\PYG{p}{))}

        \PYG{c+c1}{\PYGZsh{} Back Propagation}
        \PYG{n}{LossVector} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{LossFunctionV2}\PYG{p}{(}\PYG{n}{output}\PYG{p}{,} \PYG{n}{tempExp}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Calculating Loss}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchLoss} \PYG{o}{+=} \PYG{n}{LossVector}\PYG{o}{.}\PYG{n}{matrixVals}\PYG{p}{[}\PYG{n}{action}\PYG{p}{][}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{LossVector}\PYG{o}{.}\PYG{n}{matrixVals}\PYG{p}{[}\PYG{n}{action}\PYG{p}{][}\PYG{l+m+mi}{0}\PYG{p}{])}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{errSignal} \PYG{o}{=} \PYG{n}{LossVector}

        \PYG{c+c1}{\PYGZsh{}self.MainNetwork.BackPropagationV2(self.activations) \PYGZsh{} Back Propagating the loss}

        \PYG{c+c1}{\PYGZsh{} Do things every X steps passed}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step} \PYG{o}{\PYGZpc{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}TargetReplaceRate\PYGZdq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Replace Weights in Target Network}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{TargetNetwork}\PYG{o}{.}\PYG{n}{layers} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{layers}

        \PYG{c+c1}{\PYGZsh{} Sample Experience Replay Buffer}
        \PYG{k}{if} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}EREnabled\PYGZdq{}}\PYG{p}{]} \PYG{o+ow}{and} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step} \PYG{o}{\PYGZpc{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}ERSampleRate\PYGZdq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0} \PYG{o+ow}{and} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ExperienceReplay}\PYG{o}{.}\PYG{n}{Full}\PYG{p}{()):} 
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{SampleExperienceReplay}\PYG{p}{(}\PYG{n}{agent}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Actions to run after every Batch}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step} \PYG{o}{\PYGZpc{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DQLEpoch\PYGZdq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:} 
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cumReward}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{}self.MainNetwork.UpdateWeightsAndBiases(self.paramDictionary[\PYGZdq{}DQLEpoch\PYGZdq{}]) \PYGZsh{} Update weights and biases}

            \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}SaveWeights\PYGZdq{}}\PYG{p}{]:} \PYG{c+c1}{\PYGZsh{} Saves weights if specified}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{SaveState}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fileName}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{}Log Action}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actionTracker}\PYG{o}{.}\PYG{n}{LogDataPoint}\PYG{p}{([}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchReward}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{maxBatchReward}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchLoss}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step}\PYG{p}{])}
            \PYG{c+c1}{\PYGZsh{}self.actionTracker.LogDataPointBatch(self.dataPoints)}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataPoints} \PYG{o}{=} \PYG{p}{[]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actionTracker}\PYG{o}{.}\PYG{n}{SaveDataPoints}\PYG{p}{()}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchReward} \PYG{o}{=} \PYG{l+m+mi}{0}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{maxBatchReward} \PYG{o}{=} \PYG{l+m+mi}{0}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batchLoss} \PYG{o}{=} \PYG{l+m+mi}{0}

    \PYG{k}{def} \PYG{n+nf}{SampleExperienceReplay}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Samples the Experience Replay Buffer, Back Propagating its Findings}
        \PYG{n}{samples} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ExperienceReplay}\PYG{o}{.}\PYG{n}{Sample}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}ERSampleSize\PYGZdq{}}\PYG{p}{])}

        \PYG{k}{for} \PYG{n}{sample} \PYG{o+ow}{in} \PYG{n}{samples}\PYG{p}{:}
            \PYG{n}{postProcessedSurround} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{TileVectorPostProcess}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{.}\PYG{n}{state}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Post process the Tile Vector}
            \PYG{n}{netInput} \PYG{o}{=} \PYG{n}{postProcessedSurround}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{ForwardPropagation}\PYG{p}{(}\PYG{n}{netInput}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{activations}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Forward Prop the Main Network}

            \PYG{n}{output} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{outputVector}

            \PYG{n}{Loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{LossFunctionV2}\PYG{p}{(}\PYG{n}{output}\PYG{p}{,} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Generate Loss for the sample}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{errSignal} \PYG{o}{=} \PYG{n}{Loss}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{o}{.}\PYG{n}{BackPropagationV2}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{activations}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Back Propagate the error}

    \PYG{k}{def} \PYG{n+nf}{LossFunctionV2}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{output}\PYG{p}{,} \PYG{n}{tempExp}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{):}
        \PYG{c+c1}{\PYGZsh{} L\PYGZca{}i(W\PYGZca{}i) = ((r + y*maxQ(s\PYGZsq{},a\PYGZsq{};W\PYGZca{}i\PYGZhy{}1) \PYGZhy{} Q(s,a,W)) ** 2}
        \PYG{c+c1}{\PYGZsh{} Loss = ((Reward[] + Gamma * MaxQ(s\PYGZsq{}, a\PYGZsq{}; TNet)) \PYGZhy{} Q(s, a)[]) \PYGZca{} 2}

        \PYG{n}{Reward} \PYG{o}{=} \PYG{n}{tempExp}\PYG{o}{.}\PYG{n}{reward}
        \PYG{n}{Gamma} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DQLGamma\PYGZdq{}}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{}stateNew = agent.TileVectorPostProcess(tempExp.stateNew) \PYGZsh{} Create new state input}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{TargetNetwork}\PYG{o}{.}\PYG{n}{ForwardPropagation}\PYG{p}{(}\PYG{n}{agent}\PYG{o}{.}\PYG{n}{TileVectorPostProcess}\PYG{p}{(}\PYG{n}{tempExp}\PYG{o}{.}\PYG{n}{state}\PYG{p}{)[}\PYG{l+m+mi}{1}\PYG{p}{],} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{activations}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Apply input to Target Network}
        \PYG{n}{tempRewardVec} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{GetRewardVector}\PYG{p}{(}\PYG{n}{tempExp}\PYG{o}{.}\PYG{n}{stateNew}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DeepQLearningLayers\PYGZdq{}}\PYG{p}{][}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{])} \PYG{c+c1}{\PYGZsh{} Gets reward vector from the new state}
        \PYG{n}{maxQTNet} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{MaxQ}\PYG{p}{(}\PYG{n}{tempRewardVec}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Max of Target network}

        \PYG{n}{LossVec} \PYG{o}{=} \PYG{p}{((}\PYG{n}{Reward} \PYG{o}{+} \PYG{p}{(}\PYG{n}{Gamma} \PYG{o}{*} \PYG{n}{maxQTNet}\PYG{p}{))} \PYG{o}{\PYGZhy{}} \PYG{n}{output}\PYG{p}{)} \PYG{o}{**} \PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} Bellman Equation}
        \PYG{k}{return} \PYG{n}{LossVec}

    \PYG{k}{def} \PYG{n+nf}{SaveState}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{file}\PYG{p}{):}
        \PYG{n}{state} \PYG{o}{=} \PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{TargetNetwork}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ExperienceReplay}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step}\PYG{p}{,} 
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cumReward}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layerActivation}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{finalLayerActivation}\PYG{p}{]}
        \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}DQLearningData}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n}{file} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}.dqn\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}wb\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
            \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{LoadState}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{file}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Returns stored Neural Network data}
        \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}DQLearningData}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n}{file} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}.dqn\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}rb\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{pickle}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MainNetwork} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{TargetNetwork} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ExperienceReplay} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{step} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{cumReward} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layerActivation} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{6}\PYG{p}{]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{finalLayerActivation} \PYG{o}{=} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{]}

\PYG{k}{class} \PYG{n+nc}{NeuralNet}\PYG{p}{():} \PYG{c+c1}{\PYGZsh{} Neural Network Implementation}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{layersIn}\PYG{p}{,} \PYG{n}{params}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Constructor for a Single Neural Network}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary} \PYG{o}{=} \PYG{n}{params}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers} \PYG{o}{=} \PYG{p}{[]}

        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{layersIn}\PYG{p}{)):}
            \PYG{k}{if} \PYG{n}{i} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{Layer}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{layersIn}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{k+kc}{True}\PYG{p}{))}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{Layer}\PYG{p}{(}\PYG{n}{layersIn}\PYG{p}{[}\PYG{n}{i} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{],} \PYG{n}{layersIn}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]))}

    \PYG{k}{def} \PYG{n+nf}{ForwardPropagation}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputVector}\PYG{p}{,} \PYG{n}{activations}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Iterates through Forward Propagation}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{outputVector} \PYG{o}{=} \PYG{n}{inputVector}

        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{):}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{ForwardPropagation}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{],} \PYG{n}{activations}\PYG{p}{)}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{ForwardPropagation}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{],} \PYG{n}{activations}\PYG{p}{,} \PYG{n}{finalLayer}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{BackPropagationV2}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{activations}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Iterates through Back Propagation V2}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{):}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{BackPropagationV2}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{],} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{paramDictionary}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}DQLLearningRate\PYGZdq{}}\PYG{p}{],} \PYG{n}{activations}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{UpdateWeightsAndBiases}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{epochCount}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Update Weights and biases}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{)):}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layers}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{UpdateWeightsAndBiases}\PYG{p}{(}\PYG{n}{epochCount}\PYG{p}{)}

\PYG{k}{class} \PYG{n+nc}{Layer}\PYG{p}{():} \PYG{c+c1}{\PYGZsh{} Layer for a Neural Network}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{prevSize}\PYG{p}{,} \PYG{n}{size}\PYG{p}{,} \PYG{n}{inputLayer}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Constructor for a Layer Object}
        \PYG{k}{if} \PYG{n}{inputLayer} \PYG{o}{==} \PYG{k+kc}{False}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Additional objects if not the input layer}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightMatrix} \PYG{o}{=} \PYG{n}{Matrix}\PYG{p}{((}\PYG{n}{size}\PYG{p}{,} \PYG{n}{prevSize}\PYG{p}{),} \PYG{n}{random}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{biasVector} \PYG{o}{=} \PYG{n}{Matrix}\PYG{p}{((}\PYG{n}{size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{),} \PYG{n}{random}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightUpdates} \PYG{o}{=} \PYG{n}{Matrix}\PYG{p}{((}\PYG{n}{size}\PYG{p}{,} \PYG{n}{prevSize}\PYG{p}{))}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{biasUpdates} \PYG{o}{=} \PYG{n}{Matrix}\PYG{p}{((}\PYG{n}{size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{))}

            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{errSignal} \PYG{o}{=} \PYG{n}{Matrix}\PYG{p}{((}\PYG{n}{size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{))}
        
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sVector} \PYG{o}{=} \PYG{n}{Matrix}\PYG{p}{((}\PYG{n}{size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{))}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{outputVector} \PYG{o}{=} \PYG{n}{Matrix}\PYG{p}{((}\PYG{n}{size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{))}

    \PYG{k}{def} \PYG{n+nf}{ForwardPropagation}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{prevLayer}\PYG{p}{,} \PYG{n}{activations}\PYG{p}{,} \PYG{n}{finalLayer}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Forward Propagates the Neural Network}
        \PYG{n}{weightValueProduct} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightMatrix} \PYG{o}{*} \PYG{n}{prevLayer}\PYG{o}{.}\PYG{n}{outputVector}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sVector} \PYG{o}{=} \PYG{n}{weightValueProduct} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{biasVector}

        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{finalLayer}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Apply different activation if Output Layer }
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{outputVector} \PYG{o}{=} \PYG{n}{activations}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Activation}\PYG{p}{(}\PYG{n}{copy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sVector}\PYG{p}{))}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{outputVector} \PYG{o}{=} \PYG{n}{activations}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Activation}\PYG{p}{(}\PYG{n}{copy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sVector}\PYG{p}{))}

    \PYG{k}{def} \PYG{n+nf}{BackPropagationV2}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{prevLayer}\PYG{p}{,} \PYG{n}{lr}\PYG{p}{,} \PYG{n}{layerActivations}\PYG{p}{,} \PYG{n}{finalLayer}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} 2nd Revision of Back Propagation}
        \PYG{c+c1}{\PYGZsh{} Calculating Next Error Signal}
        \PYG{n}{halfErrSignal} \PYG{o}{=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightMatrix}\PYG{o}{.}\PYG{n}{Transpose}\PYG{p}{()} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{errSignal}\PYG{p}{)}

        \PYG{n}{zDerivative} \PYG{o}{=} \PYG{n}{layerActivations}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Derivative}\PYG{p}{(}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{prevLayer}\PYG{o}{.}\PYG{n}{sVector}\PYG{p}{))} \PYG{c+c1}{\PYGZsh{} Applying derivative functions to the output of a layerN}

        \PYG{n}{errSignal} \PYG{o}{=} \PYG{n}{halfErrSignal} \PYG{o}{*} \PYG{n}{zDerivative} \PYG{c+c1}{\PYGZsh{} Hadamard Product to get error signal for previous layer}
        \PYG{n}{prevLayer}\PYG{o}{.}\PYG{n}{errSignal} \PYG{o}{=} \PYG{n}{errSignal}

        \PYG{c+c1}{\PYGZsh{} Calculating Weight updates}
        \PYG{n}{updatedWeightVectors} \PYG{o}{=} \PYG{p}{[]}
        \PYG{k}{for} \PYG{n}{delta} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{errSignal}\PYG{o}{.}\PYG{n}{order}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]):}
            \PYG{n}{errSignal} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{errSignal}\PYG{o}{.}\PYG{n}{matrixVals}\PYG{p}{[}\PYG{n}{delta}\PYG{p}{][}\PYG{l+m+mi}{0}\PYG{p}{]}

            \PYG{n}{selectedColumn} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightMatrix}\PYG{o}{.}\PYG{n}{Transpose}\PYG{p}{()}\PYG{o}{.}\PYG{n}{SelectColumn}\PYG{p}{(}\PYG{n}{delta}\PYG{p}{)}
            \PYG{n}{updatedWeightVectors}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{selectedColumn} \PYG{o}{*} \PYG{n}{errSignal} \PYG{o}{*} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{lr}\PYG{p}{))}

        \PYG{c+c1}{\PYGZsh{} Combining the weight updates into a matrix and adding it to the weight updates Matrix}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightUpdates} \PYG{o}{+=} \PYG{n}{Matrix}\PYG{o}{.}\PYG{n}{CombineVectorsHor}\PYG{p}{(}\PYG{n}{updatedWeightVectors}\PYG{p}{)}\PYG{o}{.}\PYG{n}{Transpose}\PYG{p}{()}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{biasUpdates} \PYG{o}{+=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{errSignal} \PYG{o}{*} \PYG{n}{lr} \PYG{c+c1}{\PYGZsh{} Bias Updates}

    \PYG{k}{def} \PYG{n+nf}{UpdateWeightsAndBiases}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{epochCount}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Update Weights and Biases}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightMatrix} \PYG{o}{\PYGZhy{}=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightUpdates} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{/} \PYG{n}{epochCount}\PYG{p}{))}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{biasVector} \PYG{o}{\PYGZhy{}=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{biasUpdates} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{/} \PYG{n}{epochCount}\PYG{p}{))}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weightUpdates}\PYG{o}{.}\PYG{n}{Clear}\PYG{p}{()}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{biasUpdates}\PYG{o}{.}\PYG{n}{Clear}\PYG{p}{()}

\PYG{k}{class} \PYG{n+nc}{Experience}\PYG{p}{():} \PYG{c+c1}{\PYGZsh{} Used in Experience Replay}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{action} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{reward} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{stateNew} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Constructor for an Experience}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{state} \PYG{o}{=} \PYG{n}{state}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action} \PYG{o}{=} \PYG{n}{action}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{reward} \PYG{o}{=} \PYG{n}{reward}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{stateNew} \PYG{o}{=} \PYG{n}{stateNew}

\PYG{k}{class} \PYG{n+nc}{Deque}\PYG{p}{():} \PYG{c+c1}{\PYGZsh{} Partial Double Ended Queue Implementation}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{length}\PYG{p}{):}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{length} \PYG{o}{=} \PYG{n}{length}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{queue} \PYG{o}{=} \PYG{p}{[}\PYG{k+kc}{None} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{length}\PYG{p}{)]}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{backP} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}

    \PYG{k}{def} \PYG{n+nf}{PushFront}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{item}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Pushes item to front of Queue}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP} \PYG{o}{=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{length}

        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{queue}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP}\PYG{p}{]} \PYG{o}{!=} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{backP} \PYG{o}{=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{length}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{queue}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP}\PYG{p}{]} \PYG{o}{=} \PYG{n}{item}

    \PYG{k}{def} \PYG{n+nf}{Full}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Checks if Queue is full}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{queue}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{length} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{!=} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{k}{return} \PYG{k+kc}{True}
        \PYG{k}{return} \PYG{k+kc}{False}

    \PYG{k}{def} \PYG{n+nf}{First}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Returns Front Item from Queue}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{queue}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP}\PYG{p}{]}

    \PYG{k}{def} \PYG{n+nf}{Last}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Returns Final Item from Queue}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{queue}\PYG{p}{[(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frontP} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{length}\PYG{p}{]}

    \PYG{k}{def} \PYG{n+nf}{Sample}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{n}\PYG{p}{):} \PYG{c+c1}{\PYGZsh{} Samples N number of samples from the deque}
        \PYG{n}{temp} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{queue}
        \PYG{k}{return} \PYG{n}{random}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{temp}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
\end{Verbatim}
